{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell-1",
   "metadata": {},
   "source": [
    "# Building a ReAct Agent from Scratch\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how LLM API calls work\n",
    "- Implement the ReAct (Reasoning + Acting) framework\n",
    "- Connect an agent to real tools (Grasshopper)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for our ReAct agent\n",
    "import re      # Regular expressions - used to parse LLM output for actions\n",
    "import json    # For parsing API responses and tool results\n",
    "import os      # For accessing environment variables\n",
    "\n",
    "# Load environment variables from .env file (keeps API keys secure)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# LLM provider - Groq offers fast inference with a generous free tier\n",
    "from groq import Groq\n",
    "\n",
    "# Alternative provider - Google's Gemini models\n",
    "try:\n",
    "    from google import genai\n",
    "    from google.genai import types\n",
    "    GEMINI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEMINI_AVAILABLE = False\n",
    "    print(\"Note: google-genai not installed. Run 'pip install google-genai' to use Gemini.\")\n",
    "\n",
    "# HTTP client that supports streaming - needed for MCP's SSE protocol\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-key-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LLM Provider Configuration\n",
    "# Toggle USE_GEMINI to switch between providers. Both have free tiers!\n",
    "# - Groq: https://console.groq.com/keys (14,400 requests/day)\n",
    "# - Gemini: https://aistudio.google.com/apikey (1M token context)\n",
    "# =============================================================================\n",
    "\n",
    "USE_GEMINI = False  # Set to True for Gemini 2.5 Flash, False for Groq (Llama 3.3)\n",
    "\n",
    "if USE_GEMINI:\n",
    "    if not GEMINI_AVAILABLE:\n",
    "        raise ImportError(\"google-genai not installed! Run: pip install google-genai\")\n",
    "    \n",
    "    API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found! Add it to your .env file.\")\n",
    "    \n",
    "    client = genai.Client(api_key=API_KEY)\n",
    "    MODEL_NAME = \"gemini-2.5-flash\"\n",
    "    print(f\"Using Gemini: {MODEL_NAME}\")\n",
    "    \n",
    "    # Quick test to verify connection\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=\"Tell me a short haiku about computational design.\"\n",
    "    )\n",
    "    print(response.text)\n",
    "else:\n",
    "    API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\"GROQ_API_KEY not found! Copy .env.example to .env and add your key.\")\n",
    "    \n",
    "    client = Groq(api_key=API_KEY)\n",
    "    MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "    print(f\"Using Groq: {MODEL_NAME}\")\n",
    "    \n",
    "    # Quick test to verify connection\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Tell me a short haiku about computational design.\"}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provider-abstraction-header-1",
   "metadata": {},
   "source": [
    "### Provider Abstraction\n",
    "\n",
    "The function below wraps our LLM calls. It automatically handles both **Groq** and **Gemini** based on the `USE_GEMINI` toggle above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "call-llm-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(messages: list[dict], temperature: float = 0) -> str:\n",
    "    \"\"\"\n",
    "    Call the LLM with a list of messages.\n",
    "    \n",
    "    This abstraction layer allows us to switch providers easily.\n",
    "    The message format follows the OpenAI standard used by most providers:\n",
    "        {\"role\": \"user\" | \"assistant\" | \"system\", \"content\": \"...\"}\n",
    "    \"\"\"\n",
    "    if USE_GEMINI:\n",
    "        gemini_contents = []\n",
    "        system_instruction = None\n",
    "        \n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            \n",
    "            if role == \"system\":\n",
    "                system_instruction = content\n",
    "            elif role == \"user\":\n",
    "                gemini_contents.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=content)]))\n",
    "            elif role == \"assistant\":\n",
    "                gemini_contents.append(types.Content(role=\"model\", parts=[types.Part.from_text(text=content)]))\n",
    "        \n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=4096,\n",
    "            system_instruction=system_instruction,\n",
    "        )\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=gemini_contents,\n",
    "            config=config\n",
    "        )\n",
    "        return response.text\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-class-header-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Agent Class\n",
    "\n",
    "An agent is simply:\n",
    "1. A **system prompt** that defines its behavior\n",
    "2. A **message history** that tracks the conversation\n",
    "3. A **method to call the LLM** and append responses\n",
    "\n",
    "The key insight: **LLMs are stateless**. The agent \"remembers\" the conversation only because we maintain the message history and send it with every request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent-class-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    A conversational agent that maintains message history.\n",
    "    \n",
    "    The agent's behavior is defined by its system prompt. Each call to the agent\n",
    "    adds to the conversation history, allowing the LLM to maintain context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_prompt: str = \"\"):\n",
    "        \"\"\"Initialize the agent with an optional system prompt.\"\"\"\n",
    "        self.system_prompt = system_prompt\n",
    "        self.messages = []\n",
    "\n",
    "        # System message goes first - it sets the context for everything after\n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    def __call__(self, user_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Send a message to the agent and get a response.\n",
    "        \n",
    "        The __call__ method allows using the agent like a function: agent(\"Hello\")\n",
    "        \"\"\"\n",
    "        # Add user message with correct role\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        # Call LLM with entire conversation history\n",
    "        response = call_llm(self.messages)\n",
    "\n",
    "        # Add assistant response to history\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return response\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history while keeping the system prompt.\"\"\"\n",
    "        self.messages = []\n",
    "        if self.system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": self.system_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent-test-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Agent class - the second question tests memory\n",
    "test_agent = Agent(\"You are a helpful assistant. Be concise.\")\n",
    "print(test_agent(\"What is 2 + 2?\"))\n",
    "print(test_agent(\"What did I just ask you?\"))  # This tests memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-header-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Connecting to Real Tools (Grasshopper MCP)\n",
    "\n",
    "Now we'll connect our agent to **real tools** that can control Grasshopper.\n",
    "\n",
    "The MCP (Model Context Protocol) server exposes these tools:\n",
    "- `List_Python_Scripts` - Find script components on the canvas\n",
    "- `Get_Python_Script` - Read a script's code\n",
    "- `Edit_Python_Script` - Write/modify code\n",
    "- `Get_Python_Script_Errors` - Check for compilation errors\n",
    "\n",
    "**Important:** Make sure Grasshopper is running with the MCP server active!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcp-config-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP (Model Context Protocol) Connection\n",
    "# This allows our agent to call tools in Grasshopper via HTTP/SSE\n",
    "\n",
    "MCP_URL = \"http://127.0.0.1:8089/mcp\"  # Default Grasshopper MCP endpoint\n",
    "\n",
    "def call_mcp_tool(tool_name: str, arguments: dict = None) -> dict:\n",
    "    \"\"\"Call a tool on the Grasshopper MCP server.\"\"\"\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": 1,\n",
    "        \"method\": \"tools/call\",\n",
    "        \"params\": {\n",
    "            \"name\": tool_name,\n",
    "            \"arguments\": arguments or {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with httpx.Client(timeout=30) as http_client:\n",
    "            with http_client.stream(\"POST\", MCP_URL, json=payload) as response:\n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data = json.loads(line[6:])\n",
    "                        if \"result\" in data:\n",
    "                            return data[\"result\"]\n",
    "                        return data\n",
    "                    elif line.strip() and not line.startswith(\":\"):\n",
    "                        try:\n",
    "                            return json.loads(line)\n",
    "                        except:\n",
    "                            pass\n",
    "        return {\"error\": \"No response received\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool-wrappers-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and filter available tools from MCP server\n",
    "\n",
    "def get_available_tools() -> list:\n",
    "    \"\"\"Fetch tool definitions from the MCP server.\"\"\"\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": 1,\n",
    "        \"method\": \"tools/list\",\n",
    "        \"params\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with httpx.Client(timeout=30) as http_client:\n",
    "            with http_client.stream(\"POST\", MCP_URL, json=payload) as response:\n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data = json.loads(line[6:])\n",
    "                        if \"result\" in data and \"tools\" in data[\"result\"]:\n",
    "                            return data[\"result\"][\"tools\"]\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tools: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get all tools and filter to the Python script tools we need\n",
    "ALL_TOOLS = get_available_tools()\n",
    "\n",
    "TUTORIAL_TOOLS = [t for t in ALL_TOOLS if t[\"name\"] in [\n",
    "    \"List_Python_Scripts\",\n",
    "    \"Get_Python_Script\", \n",
    "    \"Edit_Python_Script\",\n",
    "    \"Get_Python_Script_Errors\"\n",
    "]]\n",
    "\n",
    "print(f\"Found {len(ALL_TOOLS)} total tools, using {len(TUTORIAL_TOOLS)} for this tutorial:\")\n",
    "for tool in TUTORIAL_TOOLS:\n",
    "    print(f\"  - {tool['name']}: {tool['description'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcp-test-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the MCP connection\n",
    "result = call_mcp_tool(\"List_Python_Scripts\")\n",
    "print(\"Python scripts on canvas:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "react-header-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: The ReAct Framework\n",
    "\n",
    "**ReAct** = **Re**asoning + **Act**ing\n",
    "\n",
    "The key insight: LLMs can follow a structured loop if we tell them to:\n",
    "\n",
    "1. **Thought** - Reason about what to do next\n",
    "2. **Action** - Call a tool with specific input\n",
    "3. **PAUSE** - Stop and wait for the tool result\n",
    "4. **Observation** - Receive the tool's output\n",
    "5. **Repeat** until ready to give final **Answer**\n",
    "\n",
    "The magic is in the **prompt** - structured instructions create structured behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "react-prompt-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_react_prompt(tools: list) -> str:\n",
    "    \"\"\"\n",
    "    Build a ReAct prompt from MCP tool definitions.\n",
    "    \n",
    "    The prompt has several key sections:\n",
    "    1. Role definition - who the agent is\n",
    "    2. Domain rules - Grasshopper-specific knowledge\n",
    "    3. Tool descriptions - dynamically generated from MCP\n",
    "    4. Workflow - the ReAct loop pattern\n",
    "    5. Example - shows exactly what success looks like\n",
    "    \"\"\"\n",
    "    \n",
    "    tool_docs = []\n",
    "    for tool in tools:\n",
    "        name = tool[\"name\"]\n",
    "        desc = tool[\"description\"].split('.')[0]\n",
    "        params = tool.get(\"inputSchema\", {}).get(\"properties\", {})\n",
    "        param_names = list(params.keys())\n",
    "        tool_docs.append(f\"- {name}: {desc}. Parameters: {param_names if param_names else 'none'}\")\n",
    "    \n",
    "    tools_section = \"\\n\".join(tool_docs)\n",
    "    \n",
    "    return f\"\"\"You are a Grasshopper Python scripting assistant that creates geometry.\n",
    "\n",
    "<context>\n",
    "You write Python scripts for Grasshopper (Rhino's visual programming tool).\n",
    "Scripts run inside a Python component with predefined output variables.\n",
    "</context>\n",
    "\n",
    "<grasshopper_rules>\n",
    "- Output geometry by assigning to variable 'a': a = my_points\n",
    "- Import Rhino.Geometry as rg for geometry types\n",
    "- Use rg.Point3d(x,y,z) for points, rg.Line() for lines, etc.\n",
    "- Lists of geometry display as multiple objects\n",
    "- Always assign final result to 'a' or it won't display!\n",
    "</grasshopper_rules>\n",
    "\n",
    "<tools>\n",
    "{tools_section}\n",
    "</tools>\n",
    "\n",
    "<workflow>\n",
    "1. Thought: reason about what to do\n",
    "2. Action: call a tool, then PAUSE\n",
    "3. Wait for Observation\n",
    "4. After editing code, always check for errors\n",
    "5. Only give Answer when code works\n",
    "</workflow>\n",
    "\n",
    "<format>\n",
    "Action: ToolName {{\"param\": \"value\"}}\n",
    "</format>\n",
    "\n",
    "<example>\n",
    "User: Create a spiral of 10 points\n",
    "\n",
    "Thought: Find the script component first.\n",
    "Action: List_Python_Scripts\n",
    "PAUSE\n",
    "\n",
    "Observation: [{{\"id\": \"abc-123\", \"name\": \"Task\"}}]\n",
    "\n",
    "Thought: Write code for a spiral. Must assign to 'a' for output.\n",
    "Action: Edit_Python_Script {{\"componentId\": \"abc-123\", \"code\": \"import Rhino.Geometry as rg\\\\nimport math\\\\n\\\\npoints = []\\\\nfor i in range(10):\\\\n    angle = i * 0.5\\\\n    x = math.cos(angle) * i\\\\n    y = math.sin(angle) * i\\\\n    points.append(rg.Point3d(x, y, 0))\\\\n\\\\na = points\"}}\n",
    "PAUSE\n",
    "\n",
    "Observation: Script updated\n",
    "\n",
    "Thought: Check for errors.\n",
    "Action: Get_Python_Script_Errors {{\"componentId\": \"abc-123\"}}\n",
    "PAUSE\n",
    "\n",
    "Observation: No errors\n",
    "\n",
    "Answer: Created spiral of 10 points, assigned to output 'a'.\n",
    "</example>\n",
    "\"\"\"\n",
    "\n",
    "REACT_PROMPT = build_react_prompt(TUTORIAL_TOOLS)\n",
    "print(f\"Prompt built ({len(REACT_PROMPT)} chars)\")\n",
    "print(REACT_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loop-header-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: The ReAct Execution Loop\n",
    "\n",
    "Now we need code that:\n",
    "1. Parses the LLM's output to find Actions\n",
    "2. Executes the corresponding tool\n",
    "3. Feeds the result back as an Observation\n",
    "4. Repeats until we get an Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "action-parser-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Parser - extracts tool calls from LLM output using regex\n",
    "# Format: \"Action: ToolName {\"param\": \"value\"}\"\n",
    "\n",
    "ACTION_PATTERN = re.compile(r'^Action:\\s*(\\w+)\\s*(\\{.*\\})?$', re.MULTILINE)\n",
    "\n",
    "def parse_action(response: str) -> tuple[str, dict] | None:\n",
    "    \"\"\"\n",
    "    Parse an action from the LLM response.\n",
    "    Returns: (tool_name, arguments_dict) if action found, else None\n",
    "    \"\"\"\n",
    "    match = ACTION_PATTERN.search(response)\n",
    "    if match:\n",
    "        tool_name = match.group(1)\n",
    "        args_str = match.group(2)\n",
    "        \n",
    "        args = {}\n",
    "        if args_str:\n",
    "            try:\n",
    "                args = json.loads(args_str)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not parse arguments: {args_str}\")\n",
    "        \n",
    "        return (tool_name, args)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "action-executor-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_action(tool_name: str, args: dict) -> str:\n",
    "    \"\"\"\n",
    "    Execute a tool and return the result as a string.\n",
    "    Results are converted to strings because the LLM needs text.\n",
    "    \"\"\"\n",
    "    result = call_mcp_tool(tool_name, args)\n",
    "    \n",
    "    if isinstance(result, dict):\n",
    "        if \"error\" in result:\n",
    "            return f\"Error: {result['error']}\"\n",
    "        return json.dumps(result, indent=2)\n",
    "    elif isinstance(result, list):\n",
    "        return json.dumps(result, indent=2)\n",
    "    else:\n",
    "        return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-loop-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question: str, max_turns: int = 10, verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Run the ReAct loop to answer a question.\n",
    "    \n",
    "    This orchestrates: Thought -> Action -> Observation -> repeat until Answer\n",
    "    \n",
    "    Key insight: We're having a conversation where \"user\" messages are\n",
    "    actually observations from the real world (tool results)!\n",
    "    \"\"\"\n",
    "    agent = Agent(REACT_PROMPT)\n",
    "    next_prompt = question\n",
    "\n",
    "    for turn in range(max_turns):\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Turn {turn + 1}\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "        response = agent(next_prompt)\n",
    "        if verbose:\n",
    "            print(response)\n",
    "\n",
    "        # Check for final answer\n",
    "        if \"Answer:\" in response and \"PAUSE\" not in response:\n",
    "            answer_start = response.find(\"Answer:\")\n",
    "            return response[answer_start + 7:].strip()\n",
    "\n",
    "        # Parse and execute action\n",
    "        action_result = parse_action(response)\n",
    "        if action_result:\n",
    "            tool_name, args = action_result\n",
    "            if verbose:\n",
    "                print(f\"\\n>> Executing: {tool_name}({args})\")\n",
    "\n",
    "            observation = execute_action(tool_name, args)\n",
    "            if verbose:\n",
    "                print(f\">> Result: {observation[:300]}...\")\n",
    "\n",
    "            next_prompt = f\"Observation: {observation}\"\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"\\n>> No action found, prompting to continue...\")\n",
    "            next_prompt = \"Continue with an Action or provide your Answer.\"\n",
    "\n",
    "    return \"Max turns reached.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-header-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Let's Run It!\n",
    "\n",
    "**Make sure:**\n",
    "1. Grasshopper is open\n",
    "2. You have a Python 3 Script component on the canvas\n",
    "3. The MCP server is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-execution-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different prompts!\n",
    "user_prompt = \"\"\"\n",
    "Create a spiral staircase including steps\n",
    "\"\"\"\n",
    "\n",
    "result = query(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Built\n",
    "\n",
    "1. **An Agent class** - maintains conversation history, calls the LLM\n",
    "2. **A ReAct prompt** - tells the LLM to think, act, observe, repeat\n",
    "3. **An execution loop** - parses actions, runs tools, feeds back results\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **LLMs are stateless** - we maintain state with message history\n",
    "- **Prompts are programs** - structured instructions create structured behavior  \n",
    "- **Tools extend capabilities** - connect LLMs to real systems via MCP\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Read the original ReAct paper: [Yao et al., 2022](https://arxiv.org/abs/2210.03629)\n",
    "- [DeepLearning.AI Agentic AI Course](https://www.deeplearning.ai/courses/agentic-ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus-header-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Pretty Visualization with Rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rich-query-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "\n",
    "console = Console()\n",
    "\n",
    "def extract_thought(response: str) -> str:\n",
    "    \"\"\"Extract the Thought section from LLM response.\"\"\"\n",
    "    lines = response.split('\\n')\n",
    "    thought_lines = []\n",
    "    in_thought = False\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip().startswith('Thought:'):\n",
    "            in_thought = True\n",
    "            thought_lines.append(line.replace('Thought:', '').strip())\n",
    "        elif in_thought and (line.strip().startswith('Action:') or line.strip() == 'PAUSE'):\n",
    "            break\n",
    "        elif in_thought:\n",
    "            thought_lines.append(line.strip())\n",
    "    \n",
    "    return ' '.join(thought_lines).strip()\n",
    "\n",
    "def query_pretty(question: str, max_turns: int = 10) -> str:\n",
    "    \"\"\"Run the ReAct loop with Rich visualization.\"\"\"\n",
    "    agent = Agent(REACT_PROMPT)\n",
    "    next_prompt = question\n",
    "    \n",
    "    console.print(Panel(question, title=\"[bold white]User Request[/bold white]\", border_style=\"white\"))\n",
    "\n",
    "    for turn in range(max_turns):\n",
    "        console.print(f\"\\n[bold cyan]--- Turn {turn + 1} ---[/bold cyan]\")\n",
    "\n",
    "        response = agent(next_prompt)\n",
    "        \n",
    "        thought = extract_thought(response)\n",
    "        if thought:\n",
    "            console.print(Panel(thought, title=\"[bold]Thought[/bold]\", border_style=\"blue\"))\n",
    "\n",
    "        if \"Answer:\" in response and \"PAUSE\" not in response:\n",
    "            answer_start = response.find(\"Answer:\")\n",
    "            final_answer = response[answer_start + 7:].strip()\n",
    "            console.print(Panel(final_answer, title=\"[bold]Final Answer[/bold]\", border_style=\"green\"))\n",
    "            return final_answer\n",
    "\n",
    "        action_result = parse_action(response)\n",
    "        if action_result:\n",
    "            tool_name, args = action_result\n",
    "            args_str = json.dumps(args, indent=2) if args else \"{}\"\n",
    "            console.print(Panel(\n",
    "                f\"[bold yellow]{tool_name}[/bold yellow]\\n[dim]{args_str}[/dim]\",\n",
    "                title=\"[bold]Action[/bold]\",\n",
    "                border_style=\"green\"\n",
    "            ))\n",
    "\n",
    "            observation = execute_action(tool_name, args)\n",
    "            display_obs = observation[:500] + \"...\" if len(observation) > 500 else observation\n",
    "            console.print(Panel(\n",
    "                Syntax(display_obs, \"json\", theme=\"monokai\", line_numbers=False),\n",
    "                title=\"[bold]Observation[/bold]\",\n",
    "                border_style=\"yellow\"\n",
    "            ))\n",
    "\n",
    "            next_prompt = f\"Observation: {observation}\"\n",
    "        else:\n",
    "            next_prompt = \"Continue with an Action or provide your Answer.\"\n",
    "\n",
    "    return \"Max turns reached.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rich-demo-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the pretty version!\n",
    "query_pretty(\"Create a grid of 5x5 points\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
