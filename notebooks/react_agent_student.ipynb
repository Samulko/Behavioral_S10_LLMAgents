{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a ReAct Agent\n",
    "\n",
    "Build an LLM agent that controls Grasshopper using the **ReAct** (Reasoning + Acting) framework.\n",
    "\n",
    "**What you'll build:**\n",
    "- An `Agent` class that maintains conversation history\n",
    "- Connection to Grasshopper via MCP (Model Context Protocol)  \n",
    "- A ReAct loop: Thought ‚Üí Action ‚Üí Observation ‚Üí repeat\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, json, os\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "\n",
    "# Provider imports - both are available\n",
    "from groq import Groq\n",
    "\n",
    "# For Gemini support (install with: pip install google-genai)\n",
    "try:\n",
    "    from google import genai\n",
    "    from google.genai import types\n",
    "    GEMINI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEMINI_AVAILABLE = False\n",
    "    print(\"Note: google-genai not installed. Run 'pip install google-genai' to use Gemini.\")\n",
    "\n",
    "load_dotenv()  # Load API keys from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Groq: llama-3.3-70b-versatile\n",
      "Metal minds learn slow\n",
      "Actions born of code and trial\n",
      "Robots think, adapt rise\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PROVIDER TOGGLE - Change this to switch between Groq and Gemini\n",
    "# =============================================================================\n",
    "USE_GEMINI = False  # Set to True to use Gemini 2.5 Flash, False for Groq (Llama 3.3)\n",
    "\n",
    "# API Keys:\n",
    "# - Groq (FREE): https://console.groq.com/keys\n",
    "# - Gemini (FREE): https://aistudio.google.com/apikey\n",
    "\n",
    "if USE_GEMINI:\n",
    "    if not GEMINI_AVAILABLE:\n",
    "        raise ImportError(\"google-genai not installed! Run: pip install google-genai\")\n",
    "    \n",
    "    API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found! Add it to your .env file.\")\n",
    "    \n",
    "    client = genai.Client(api_key=API_KEY)\n",
    "    MODEL_NAME = \"gemini-2.5-flash\"\n",
    "    print(f\"Using Gemini: {MODEL_NAME}\")\n",
    "    \n",
    "    # Quick test\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=\"Tell me a haiku about Behavioral robotics\"\n",
    "    )\n",
    "    print(response.text)\n",
    "else:\n",
    "    API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\"GROQ_API_KEY not found! Copy .env.example to .env and add your key.\")\n",
    "    \n",
    "    client = Groq(api_key=API_KEY)\n",
    "    MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "    print(f\"Using Groq: {MODEL_NAME}\")\n",
    "    \n",
    "    # Quick test\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Tell me a haiku about Behavioral robotics\"}],\n",
    "        temperature=1\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Wrapper\n",
    "\n",
    "Wrap the API call in a function. To switch providers (OpenAI, Anthropic, etc.), just change this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(messages: list[dict], temperature: float = 0) -> str:\n",
    "    \"\"\"Call the LLM with a list of messages, return response text.\n",
    "    \n",
    "    Works with both Groq and Gemini based on USE_GEMINI toggle.\n",
    "    \"\"\"\n",
    "    if USE_GEMINI:\n",
    "        # Convert messages to Gemini format\n",
    "        gemini_contents = []\n",
    "        system_instruction = None\n",
    "        \n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            \n",
    "            if role == \"system\":\n",
    "                system_instruction = content\n",
    "            elif role == \"user\":\n",
    "                gemini_contents.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=content)]))\n",
    "            elif role == \"assistant\":\n",
    "                gemini_contents.append(types.Content(role=\"model\", parts=[types.Part.from_text(text=content)]))\n",
    "        \n",
    "        # Configure generation\n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=4096,\n",
    "            system_instruction=system_instruction,\n",
    "        )\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=gemini_contents,\n",
    "            config=config\n",
    "        )\n",
    "        return response.text\n",
    "    else:\n",
    "        # Groq (OpenAI-compatible)\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Agent Class\n",
    "\n",
    "An agent is just:\n",
    "1. A **system prompt** that defines behavior\n",
    "2. A **message history** (list of messages)\n",
    "3. A method to **call the LLM** and track the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 18)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:18\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef __call__(self, user_message: str) -> str:\u001b[39m\n                                                 ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "\n",
    "## Part 2: Agent Class\n",
    "\n",
    "        \"\"\"Initialize with optional system prompt.\"\"\"\n",
    "        # TODO: Store system_prompt as self.system_prompt\n",
    "        # TODO: Initialize self.messages = []\n",
    "        # TODO: If system_prompt provided, append {\"role\": \"system\", \"content\": system_prompt}\n",
    "\n",
    "    def __init__(self, systetm_prompt: str = \"\")\n",
    "        self.system_prompt = system_prompt\n",
    "        self.messages = []\n",
    "\n",
    "        if system_system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, user_message: str) -> str:\n",
    "        \"\"\"Send a message and get a response.\"\"\"\n",
    "        # TODO: Append user message to self.messages\n",
    "        # TODO: Call call_llm(self.messages) to get response\n",
    "        # TODO: Append assistant response to self.messages\n",
    "        # TODO: Return the response\n",
    "        \n",
    "        self.messages.append({\"role\": \"system\", \"content\": user_message})\n",
    "\n",
    "        response = call_llm(self.messages)\n",
    "\n",
    "        self.messages.append({\"role\": \"system\", \"content\": response})\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear history but keep system prompt.\"\"\"\n",
    "        # TODO: Reset self.messages to []\n",
    "        # TODO: Re-add system prompt if it exists\n",
    "        self.message[]\n",
    "        if self.system_prompt:\n",
    "            self.message.append({\"role\": \"system\", \"content\": self.system_prompt}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your Agent - should remember the conversation\n",
    "agent = Agent(\"You are helpful. Be concise.\")\n",
    "print(agent(\"What is 2+2?\"))\n",
    "print(agent(\"What did I just ask?\"))  # Tests memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: MCP Tools (Grasshopper)\n",
    "\n",
    "Tools we'll use:\n",
    "- `List_Python_Scripts` - Find script components\n",
    "- `Get_Python_Script` - Read script code\n",
    "- `Edit_Python_Script` - Write/modify code  \n",
    "- `Get_Python_Script_Errors` - Check for errors\n",
    "\n",
    "Make sure Grasshopper is running with MCP server active!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_URL = \"http://127.0.0.1:8089/mcp\"\n",
    "\n",
    "def call_mcp_tool(tool_name: str, arguments: dict = None) -> dict:\n",
    "    \"\"\"Call a tool on the Grasshopper MCP server using SSE streaming.\"\"\"\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\", \"id\": 1,\n",
    "        \"method\": \"tools/call\",\n",
    "        \"params\": {\"name\": tool_name, \"arguments\": arguments or {}}\n",
    "    }\n",
    "    try:\n",
    "        with httpx.Client(timeout=30) as client:\n",
    "            with client.stream(\"POST\", MCP_URL, json=payload) as response:\n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data = json.loads(line[6:])\n",
    "                        return data.get(\"result\", data)\n",
    "        return {\"error\": \"No response\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_tools() -> list:\n",
    "    \"\"\"Fetch available tools from the MCP server.\"\"\"\n",
    "    payload = {\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/list\", \"params\": {}}\n",
    "    try:\n",
    "        with httpx.Client(timeout=30) as client:\n",
    "            with client.stream(\"POST\", MCP_URL, json=payload) as response:\n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data = json.loads(line[6:])\n",
    "                        if \"result\" in data and \"tools\" in data[\"result\"]:\n",
    "                            return data[\"result\"][\"tools\"]\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch tools and filter to the ones we need\n",
    "ALL_TOOLS = get_available_tools()\n",
    "TUTORIAL_TOOLS = [t for t in ALL_TOOLS if t[\"name\"] in [\n",
    "    \"List_Python_Scripts\", \"Get_Python_Script\", \"Edit_Python_Script\", \"Get_Python_Script_Errors\"\n",
    "]]\n",
    "print(f\"Found {len(TUTORIAL_TOOLS)} tools: {[t['name'] for t in TUTORIAL_TOOLS]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: List Python scripts on the Grasshopper canvas\n",
    "call_mcp_tool(\"List_Python_Scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: The ReAct Prompt\n",
    "\n",
    "**ReAct** = Reasoning + Acting. The LLM follows a loop:\n",
    "\n",
    "1. **Thought** - Reason about what to do\n",
    "2. **Action** - Call a tool, then PAUSE\n",
    "3. **Observation** - Receive tool result\n",
    "4. **Repeat** until ready to give final **Answer**\n",
    "\n",
    "The prompt tells the LLM exactly how to behave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_react_prompt(tools: list) -> str:\n",
    "    \"\"\"Build the ReAct system prompt with tool descriptions.\"\"\"\n",
    "    tool_docs = []\n",
    "    for tool in tools:\n",
    "        name = tool[\"name\"]\n",
    "        desc = tool[\"description\"].split('.')[0]\n",
    "        params = list(tool.get(\"inputSchema\", {}).get(\"properties\", {}).keys())\n",
    "        tool_docs.append(f\"- {name}: {desc}. Params: {params or 'none'}\")\n",
    "    \n",
    "    return f\"\"\"You are a Grasshopper Python scripting assistant.\n",
    "\n",
    "<rules>\n",
    "- Output geometry by assigning to 'a': a = points\n",
    "- Import Rhino.Geometry as rg\n",
    "- Always check for errors after editing code\n",
    "</rules>\n",
    "\n",
    "<tools>\n",
    "{chr(10).join(tool_docs)}\n",
    "</tools>\n",
    "\n",
    "<workflow>\n",
    "Thought: reason about what to do\n",
    "Action: ToolName {{\"param\": \"value\"}}\n",
    "PAUSE\n",
    "(wait for Observation)\n",
    "Answer: when done\n",
    "</workflow>\n",
    "\n",
    "<example>\n",
    "User: Create 10 points in a spiral\n",
    "\n",
    "Thought: Find the script component.\n",
    "Action: List_Python_Scripts\n",
    "PAUSE\n",
    "\n",
    "Observation: [{{\"id\": \"abc-123\", \"name\": \"Task\"}}]\n",
    "\n",
    "Thought: Write spiral code.\n",
    "Action: Edit_Python_Script {{\"componentId\": \"abc-123\", \"code\": \"import Rhino.Geometry as rg\\\\nimport math\\\\npoints = [rg.Point3d(math.cos(i)*i, math.sin(i)*i, 0) for i in range(10)]\\\\na = points\"}}\n",
    "PAUSE\n",
    "\n",
    "Observation: Script updated\n",
    "\n",
    "Thought: Check errors.\n",
    "Action: Get_Python_Script_Errors {{\"componentId\": \"abc-123\"}}\n",
    "PAUSE\n",
    "\n",
    "Observation: No errors\n",
    "\n",
    "Answer: Created spiral of 10 points.\n",
    "</example>\n",
    "\"\"\"\n",
    "\n",
    "REACT_PROMPT = build_react_prompt(TUTORIAL_TOOLS)\n",
    "print(f\"Prompt length: {len(REACT_PROMPT)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Prompt Works\n",
    "\n",
    "Notice the key elements:\n",
    "1. **Role definition** - \"You are a Grasshopper scripting assistant\"\n",
    "2. **Loop structure** - Explicit Thought ‚Üí Action ‚Üí PAUSE ‚Üí Observation cycle\n",
    "3. **Tool descriptions** - Exact syntax with examples\n",
    "4. **Workflow rules** - \"ALWAYS check for errors\"\n",
    "5. **Example session** - Shows the expected format\n",
    "\n",
    "The LLM follows this because it's trained to follow instructions.\n",
    "The more specific and structured your prompt, the more reliable the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: The Execution Loop\n",
    "\n",
    "Now we need code to:\n",
    "1. Parse actions from LLM output\n",
    "2. Execute the tool\n",
    "3. Feed the result back as an Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to match: Action: ToolName {\"param\": \"value\"}\n",
    "ACTION_PATTERN = re.compile(r'^Action:\\s*(\\w+)\\s*(\\{.*\\})?$', re.MULTILINE)\n",
    "\n",
    "def parse_action(response: str) -> tuple[str, dict] | None:\n",
    "    \"\"\"Parse an action from LLM response. Returns (tool_name, args) or None.\"\"\"\n",
    "    # TODO: Use ACTION_PATTERN.search(response) to find a match\n",
    "    # TODO: If match: extract tool_name from group(1), args JSON from group(2)\n",
    "    # TODO: Parse args with json.loads() if present, else empty dict\n",
    "    # TODO: Return (tool_name, args) or None if no match\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_action(tool_name: str, args: dict) -> str:\n",
    "    \"\"\"Execute an MCP tool and return the result as a string.\"\"\"\n",
    "    # TODO: Call call_mcp_tool(tool_name, args)\n",
    "    # TODO: If result has \"error\" key, return f\"Error: {result['error']}\"\n",
    "    # TODO: Otherwise return json.dumps(result, indent=2)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question: str, max_turns: int = 10, verbose: bool = True) -> str:\n",
    "    \"\"\"Run the ReAct loop until we get an Answer.\"\"\"\n",
    "    # TODO: Create Agent with REACT_PROMPT\n",
    "    # TODO: Set next_prompt = question\n",
    "    # TODO: Loop up to max_turns:\n",
    "    #   - Get response from agent(next_prompt)\n",
    "    #   - If verbose: print turn number and response\n",
    "    #   - If \"Answer:\" in response (without \"PAUSE\"): extract and return the answer\n",
    "    #   - Parse action with parse_action(response)\n",
    "    #   - If action found: execute it, set next_prompt = f\"Observation: {result}\"\n",
    "    #   - If no action: set next_prompt = \"Continue with an Action or provide your Answer.\"\n",
    "    # TODO: Return \"Max turns reached.\" if loop ends\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Run It!\n",
    "\n",
    "Make sure Grasshopper is open with `examples/task_template.gh` loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different prompts!\n",
    "query(\"Create a spiral staircase with steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You built a ReAct agent that:\n",
    "- Maintains conversation history\n",
    "- Follows the Thought ‚Üí Action ‚Üí Observation loop  \n",
    "- Connects to real tools via MCP\n",
    "\n",
    "**Learn more:**\n",
    "- [ReAct Paper (Yao et al., 2022)](https://arxiv.org/abs/2210.03629)\n",
    "- [DeepLearning.AI Agentic AI Course](https://www.deeplearning.ai/courses/agentic-ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Pretty Output\n",
    "\n",
    "A nicer version using Rich library with color-coded panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "\n",
    "console = Console()\n",
    "\n",
    "def extract_thought(response: str) -> str:\n",
    "    lines = response.split('\\n')\n",
    "    thought_lines = []\n",
    "    in_thought = False\n",
    "    for line in lines:\n",
    "        if line.strip().startswith('Thought:'):\n",
    "            in_thought = True\n",
    "            thought_lines.append(line.replace('Thought:', '').strip())\n",
    "        elif in_thought and (line.strip().startswith('Action:') or line.strip() == 'PAUSE'):\n",
    "            break\n",
    "        elif in_thought:\n",
    "            thought_lines.append(line.strip())\n",
    "    return ' '.join(thought_lines).strip()\n",
    "\n",
    "def query_pretty(question: str, max_turns: int = 10) -> str:\n",
    "    agent = Agent(REACT_PROMPT)\n",
    "    next_prompt = question\n",
    "    console.print(Panel(question, title=\"[bold white]Request[/bold white]\", border_style=\"white\"))\n",
    "\n",
    "    for turn in range(max_turns):\n",
    "        console.print(f\"\\n[bold cyan]‚îÅ‚îÅ‚îÅ Turn {turn + 1} ‚îÅ‚îÅ‚îÅ[/bold cyan]\")\n",
    "        response = agent(next_prompt)\n",
    "        \n",
    "        thought = extract_thought(response)\n",
    "        if thought:\n",
    "            console.print(Panel(thought, title=\"[bold]üí≠ Thought[/bold]\", border_style=\"blue\"))\n",
    "\n",
    "        if \"Answer:\" in response and \"PAUSE\" not in response:\n",
    "            final_answer = response[response.find(\"Answer:\") + 7:].strip()\n",
    "            console.print(Panel(final_answer, title=\"[bold]‚úÖ Answer[/bold]\", border_style=\"green\"))\n",
    "            return final_answer\n",
    "\n",
    "        action_result = parse_action(response)\n",
    "        if action_result:\n",
    "            tool_name, args = action_result\n",
    "            console.print(Panel(f\"[yellow]{tool_name}[/yellow] {json.dumps(args)}\", title=\"[bold]‚ö° Action[/bold]\", border_style=\"green\"))\n",
    "            observation = execute_action(tool_name, args)\n",
    "            console.print(Panel(Syntax(observation[:500], \"json\", theme=\"monokai\"), title=\"[bold]üëÅ Observation[/bold]\", border_style=\"yellow\"))\n",
    "            next_prompt = f\"Observation: {observation}\"\n",
    "        else:\n",
    "            next_prompt = \"Continue with an Action or provide your Answer.\"\n",
    "\n",
    "    return \"Max turns reached.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pretty(\"Create a 5x5 grid of points\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
